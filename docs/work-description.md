
## Crawling and Generating Lucene Index

#### Step. 1 - Crawl all web Urls from AOL dataset
  * List of all URLs can be found [here](https://drive.google.com/a/virginia.edu/file/d/0B8ZGlkqDw7hFNkc0c0p1OVF2YTA/view).

#### Step. 2 - Creating Lucene Index
  * Create the lucene index from all crawled data by running [Index.java](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/src/edu/virginia/cs/index/Indexer.java).
  * For AOL dataset, lucene index can be found [here](https://drive.google.com/a/virginia.edu/folderview?id=0B8ZGlkqDw7hFV2trYW9ETmo4cGc&usp=sharing).

## Generating Topic Model

Requirement: [BBC dataset](http://mlg.ucd.ie/datasets/bbc.html), [Binary for LDA-C](https://github.com/magsilva/lda-c/tree/master/bin), [Settings file](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/settings.txt) to set parameters for LDA

#### Step. 1 - Constructing dictionary and document record
  * Run [BuildTopicModel.java](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/src/edu/virginia/cs/model/BuildTopicModel.java).
  * Dictionary and document record will be created using BBC dataset.
    + BBC dataset should be located at - **project_root_directory]/data/bbc/**.
    + Dictionary ("dictionary.txt") and document record ("documentRecord.dat") will be placed at the project root directory.
  * This step also generated "dictionaryWithFrequency.txt" file and placed at the project root directory.

##### File description:
  * dictionary.txt: This file contains unigrams and bigrams found in the BBC dataset. Each line contains one unigram/bigram.
  * documentRecord.dat: This file contains one line per BBC document. Each line looks like the following.
  <pre>
   350 501:1 530:1 723:1 443:1 598:1 621:1 707:1 561:1 591:1 490:1 483:1 487:1 438:1 688:1 573:1 604:1 471:2
   413:1 410:1 3:1 632:1 569:1 488:1 499:1 599:1 439:1 401:7 595:2 713:1 526:1 648:1 179:1 626:1 518:3 655:1
  </pre>
  The first numeric value represents the total number of unique terms found in the document. Then all <code>x:y</code> value represents <code>term index in the dictionary:term frequency</code>. All the values are separated by space.
  * dictionaryWithFrequency.txt: This file contains unigrams and bigrams with their total term frequency over the entire BBC dataset.     Each line contains one unigram/bigram and corresponding total term frequency seperated by space.

#### Step. 2 - Generate the topic model using LDA-C

  * Double click the [run.bat](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/run-lda.bat) file (for windows environment), topic model will be generated and stored in **project_root_directory]/topic_model/** folder.
  * Command written in [run.bat](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/run-lda.bat) file is **lda-win64 est 0.6 5 settings.txt documentRecord.dat seeded topic_model**.
  * Third parameter value ("5") in the command represents "number of topics" for the topic model.
  * [settings.txt](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/settings.txt) file should contain all required parameter values.

## Generating Pre-requisite Data

#### Step. 1 - Generate Topical Word Distribution

 * Probability distribution of all words for each topic can be generated by running [GenerateTopicWords.java](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/src/edu/virginia/cs/model/GenerateTopicWords.java) program.
 * This is required for cover query generation which is the core part of this privacy presercing IR model.

#### Step. 2 - Generate User Search Logs

 * User profiles are a list of user submitted query and their corresponding clicked document.
 * Top N user profiles are generated from AOL dataset to evaluate the model. Top users mean the users with maximum search history.
 * User profiles can be generated by running [ProfileBuilder.java](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/src/edu/virginia/cs/parser/ProfileBuilder.java) program.
 * User profiles need to be placed in the **project_root_directory]/data/user_profiles/** folder.

#### Step. 3 - Generate Reference Model

 * Reference model is required for smoothing purpose at different times.
 * To generate reference model, [ReferenceModel.java](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/src/edu/virginia/cs/user/ReferenceModel.java) program can be used.
 * Reference model is created over all user submitted queries and their corresponding clicked documents.

## Evaluating Privacy-Preseving-IR Model

The entire evaluation procedure on a single user works as follows.
 * Loading the reference model and generate the judgements.
  + Judgements are required to measure search effectiveness (Mean Average Precision).
 * Generate **k** cover queries for each user query.
 * Each user query along with the **k** cover queries are submitted to the search engine. Search effectiveness is measured only for the true user query.
 * [Evaluate.java](https://github.com/wasiuva/Privacy-Preserving-IR/blob/master/src/edu/virginia/cs/eval/Evaluate.java) program follows the above mentioned steps.
 
##### Additional Information

* The model is tested for both personalization and non-personalization settings.
* Privacy preservation is evaluated through KL-Divergence.

